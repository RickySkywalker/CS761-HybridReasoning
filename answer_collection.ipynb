{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9763727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 22:00:37 __init__.py:190] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from transformers import AutoTokenizer\n",
    "import os\n",
    "from utils import utils\n",
    "import re\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"9\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "781a5a41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 22:00:48 config.py:542] This model supports multiple tasks: {'generate', 'classify', 'reward', 'embed', 'score'}. Defaulting to 'generate'.\n",
      "WARNING 05-05 22:00:48 arg_utils.py:1135] Chunked prefill is enabled by default for models with max_model_len > 32K. Currently, chunked prefill might not work with some features or models. If you encounter any issues, please disable chunked prefill by setting --enable-chunked-prefill=False.\n",
      "INFO 05-05 22:00:48 config.py:1556] Chunked prefill is enabled with max_num_batched_tokens=2048.\n",
      "INFO 05-05 22:00:48 llm_engine.py:234] Initializing a V0 LLM engine (v0.7.2) with config: model='./data/models/Qwen3-8B', speculative_config=None, tokenizer='./data/models/Qwen3-8B', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=40960, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=./data/models/Qwen3-8B, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={\"splitting_ops\":[],\"compile_sizes\":[],\"cudagraph_capture_sizes\":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"max_capture_size\":256}, use_cached_outputs=False, \n",
      "INFO 05-05 22:00:50 cuda.py:230] Using Flash Attention backend.\n",
      "INFO 05-05 22:00:50 model_runner.py:1110] Starting to load model ./data/models/Qwen3-8B...\n",
      "WARNING 05-05 22:00:50 utils.py:69] Qwen3ForCausalLM has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.\n",
      "INFO 05-05 22:00:50 transformers.py:132] Using Transformers backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c32cedcb34f5484783e8e0ebb358288d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading safetensors checkpoint shards:   0% Completed | 0/5 [00:00<?, ?it/s]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 22:00:54 model_runner.py:1115] Loading model weights took 15.2585 GB\n",
      "INFO 05-05 22:00:55 worker.py:267] Memory profiling takes 0.77 seconds\n",
      "INFO 05-05 22:00:55 worker.py:267] the current vLLM instance can use total_gpu_memory (47.41GiB) x gpu_memory_utilization (0.90) = 42.67GiB\n",
      "INFO 05-05 22:00:55 worker.py:267] model weights take 15.26GiB; non_torch_memory takes 0.06GiB; PyTorch activation peak memory takes 1.40GiB; the rest of the memory reserved for KV Cache is 25.95GiB.\n",
      "INFO 05-05 22:00:55 executor_base.py:110] # CUDA blocks: 11811, # CPU blocks: 1820\n",
      "INFO 05-05 22:00:55 executor_base.py:115] Maximum concurrency for 40960 tokens per request: 4.61x\n",
      "INFO 05-05 22:00:56 model_runner.py:1434] Capturing cudagraphs for decoding. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI. If out-of-memory error occurs during cudagraph capture, consider decreasing `gpu_memory_utilization` or switching to eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graph shapes: 100%|██████████| 35/35 [00:15<00:00,  2.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 05-05 22:01:12 model_runner.py:1562] Graph capturing finished in 16 secs, took 0.71 GiB\n",
      "INFO 05-05 22:01:12 llm_engine.py:431] init engine (profile, create kv cache, warmup model) took 18.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"./data/models/Qwen3-8B\"\n",
    "model = LLM(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c6b2446",
   "metadata": {},
   "outputs": [],
   "source": [
    "math_500 = utils.read_from_json(\"./data/proved_Math-500.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "efb3420c",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_str = math_500[0][\"generated_proof_results\"][0][\"response_proof_generation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ae5ae9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "match = re.search(r'<think>(.*?)(</think>|$)', test_str, re.DOTALL)\n",
    "if match:\n",
    "    long_cot_content = match.group(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2783d782",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['problem', 'solution', 'answer', 'subject', 'level', 'unique_id', 'Name', 'translation_generated_results', 'autoformalization_generated_results', 'generated_proof_results'])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_500[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "36b7bf09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Find the answer to the following question in the provided long CoT content. Your answer should be in \\boxed\\{\\} format.\n",
      "\n",
      "Here is the question:\n",
      "Compute $58_9 - 18_9.$ Express your answer in base $9.$\n",
      "\n",
      "Here is the long CoT content:\n",
      "\n",
      "# Solving the Base 9 Subtraction Problem in Lean 4\n",
      "\n",
      "We need to compute $58_9 - 18_9$ and express the answer in base 9. Let's first convert these numbers to base 10 to perform the subtraction, and then convert back to base 9.\n",
      "\n",
      "The number $58_9$ in base 10 is $5 \\cdot 9^1 + 8 \\cdot 9^0 = 5 \\cdot 9 + 8 \\cdot 1 = 45 + 8 = 53$.\n",
      "\n",
      "The number $18_9$ in base 10 is $1 \\cdot 9^1 + 8 \\cdot 9^0 = 1 \\cdot 9 + 8 \\cdot 1 = 9 + 8 = 17$.\n",
      "\n",
      "Subtracting these gives $53 - 17 = 36$.\n",
      "\n",
      "Now we need to convert $36_{10}$ back to base 9:\n",
      "- $36 \\div 9 = 4$ remainder $0$\n",
      "- $4 \\div 9 = 0$ remainder $4$\n",
      "\n",
      "So $36_{10} = 40_9$.\n",
      "\n",
      "Let's translate this into Lean 4:\n",
      "\n",
      "```tactics\n",
      "import Mathlib\n",
      "import Aesop\n",
      "\n",
      "set_option maxHeartbeats 0\n",
      "\n",
      "open BigOperators Real Nat Topology Rat\n",
      "```\n",
      "\n",
      "First, we'll define the theorem statement. We have two numbers $a$ and $b$ where:\n",
      "- $a$ is $58_9$, so in base 10, $a = 5 \\cdot 9 + 8 = 53$\n",
      "- $b$ is $18_9$, so in base 10, $b = 1 \\cdot 9 + 8 = 17$\n",
      "\n",
      "We need to prove that $a - b$ in base 9 is $40_9$, which is $4 \\cdot 9 + 0 = 36$ in base 10.\n",
      "\n",
      "```tactics\n",
      "/-- Compute $58_9 - 18_9.$ Express your answer in base $9.$ -/\n",
      "theorem math500_number_theory_46_0 {a b : ℕ} (ha : Nat.digits 9 a = [8, 5]) (hb : Nat.digits 9 b = [8, 1]) : ∃ c, Nat.digits 9 (a - b) = c := by\n",
      "```\n",
      "\n",
      "Now we need to prove that there exists some natural number $c$ such that when we compute $a - b$ and convert to base 9, we get $c$. \n",
      "\n",
      "To solve this, we need to:\n",
      "1. Convert $a$ from base 9 representation $[8, 5]$ to base 10\n",
      "2. Convert $b$ from base 9 representation $[8, 1]$ to base 10\n",
      "3. Subtract them in base 10\n",
      "4. Convert the result back to base 9\n",
      "5. Show that this is indeed the representation of $a - b$ in base 9\n",
      "\n",
      "Let's compute each step:\n",
      "- $a = 5 \\cdot 9^1 + 8 \\cdot 9^0 = 45 + 8 = 53$\n",
      "- $b = 1 \\cdot 9^1 + 8 \\cdot 9^0 = 9 + 8 = 17$\n",
      "- $a - b = 53 - 17 = 36$\n",
      "- Convert $36$ to base 9: $36 \\div 9 = 4$ remainder $0$, $4 \\div 9 = 0$ remainder $4$, so $40_9$\n",
      "\n",
      "In Lean, we need to show that $Nat.digits 9 (a - b) = [0, 4]$. \n",
      "\n",
      "To prove this, we'll:\n",
      "1. Determine the actual values of $a$ and $b$ from their base 9 digits\n",
      "2. Compute $a - b$\n",
      "3. Show that $Nat.digits 9 (a - b) = [0, 4]$\n",
      "\n",
      "Let's continue with the proof:\n",
      "\n",
      "```tactics\n",
      "have ha10 : a = 53 := by\n",
      "  rw [← Nat.ofDigits_digits 9 a]\n",
      "  simp [ha, Nat.ofDigits]\n",
      "```\n",
      "\n",
      "The proof is using the fact that $a$ in base 9 is $[8, 5]$, which means $a = 5 \\cdot 9^1 + 8 \\cdot 9^0 = 53$. The `Nat.ofDigits_digits` lemma relates a number to its digit representation, and then we substitute $a$'s digits and compute the value with `simp`.\n",
      "\n",
      "Similarly for $b$:\n",
      "\n",
      "```tactics\n",
      "have hb10 : b = 17 := by\n",
      "  rw [← Nat.ofDigits_digits 9 b]\n",
      "  simp [hb, Nat.ofDigits]\n",
      "```\n",
      "\n",
      "Now we have $a = 53$ and $b = 17$. Their difference is $53 - 17 = 36$. We need to show that $36$ in base 9 is $40_9$, which is $4 \\cdot 9^1 + 0 \\cdot 9^0 = 36$.\n",
      "\n",
      "To complete the proof, we need to show that $Nat.digits 9 (a - b) = [0, 4]$. \n",
      "\n",
      "Let's compute $a - b$ in Lean:\n",
      "\n",
      "```tactics\n",
      "use [0, 4]\n",
      "rw [ha10, hb10]\n",
      "```\n",
      "\n",
      "Now we need to show that $Nat.digits 9 (53 - 17) = [0, 4]$, which is $Nat.digits 9 36 = [0, 4]$. \n",
      "\n",
      "To verify this directly:\n",
      "- $36 \\div 9 = 4$ remainder $0$\n",
      "- $4 \\div 9 = 0$ remainder $4$\n",
      "- So $36_{10} = 40_9$, which is $[0, 4]$ in Lean's digit representation (least significant digit first)\n",
      "\n",
      "Let's finish the proof:\n",
      "\n",
      "```tactics\n",
      "norm_num\n",
      "```\n",
      "\n",
      "The `norm_num` tactic is computing $53 - 17 = 36$, and then verifying that $Nat.digits 9 36 = [0, 4]$. \n",
      "\n",
      "To recap: \n",
      "- $36$ in base 9 is computed as $4 \\cdot 9^1 + 0 \\cdot 9^0$\n",
      "- The digits in base 9 are $[0, 4]$ (least significant digit first)\n",
      "- `Nat.digits` produces digits in little-endian order, so $0$ comes before $4$\n",
      "- $36$ in base 9 is $40_9$, so $Nat.digits 9 36 = [0, 4]$\n",
      "- The `norm_num` tactic is able to compute this directly\n",
      "\n",
      "Therefore, we have shown that $58_9 - 18_9 = 40_9$.\n",
      "<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Find the answer to the following question in the provided long CoT content. Your answer should be in \\\\boxed\\{\\} format.\n",
    "\n",
    "Here is the question:\n",
    "\"\"\" + math_500[0][\"problem\"] + f\"\"\"\n",
    "\n",
    "Here is the long CoT content:\n",
    "{long_cot_content}\"\"\"\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True, \n",
    "    enable_thinking=False\n",
    ")\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ccf65d71",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:06<00:00,  6.44s/it, est. speed input: 251.27 toks/s, output: 34.17 toks/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The question asks to compute $58_9 - 18_9$ and express the answer in base 9.\n",
      "\n",
      "First, we convert both numbers from base 9 to base 10:\n",
      "- $58_9 = 5 \\cdot 9^1 + 8 \\cdot 9^0 = 45 + 8 = 53$\n",
      "- $18_9 = 1 \\cdot 9^1 + 8 \\cdot 9^0 = 9 + 8 = 17$\n",
      "\n",
      "Next, we subtract the base 10 values:\n",
      "$$\n",
      "53 - 17 = 36\n",
      "$$\n",
      "\n",
      "Now, we convert $36_{10}$ back to base 9:\n",
      "- $36 \\div 9 = 4$ remainder $0$\n",
      "- $4 \\div 9 = 0$ remainder $4$\n",
      "\n",
      "Thus, $36_{10} = 40_9$.\n",
      "\n",
      "Therefore, the answer is:\n",
      "\n",
      "$$\n",
      "\\boxed{40}\n",
      "$$\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "sampling_params = SamplingParams(temperature=0.6, top_p=0.95, max_tokens=8096)\n",
    "output = model.generate(text, sampling_params=sampling_params)\n",
    "output_text = output[0].outputs[0].text\n",
    "print(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "213279c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40\n"
     ]
    }
   ],
   "source": [
    "matches = re.findall(r'\\\\boxed\\{(.*?)\\}', output_text)\n",
    "\n",
    "if matches:\n",
    "    last_boxed = matches[-1]\n",
    "    print(last_boxed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "941ef1e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'40_9'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "math_500[0][\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb2e4ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
